% Dear Author,

% fill the fields marked with dots,

% please comments or remarks: dcs0@inf.elte.hu

%

%

\documentclass[11pt,a4paper,twoside]{article}
\usepackage{latexsym,amssymb}

\usepackage{doc}
\usepackage{scalerel}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{amsmath}

%% sorry, but it's very inconvenient to not define commands.
%% I undefine them at the bottom of the file
\newcommand{\msf}[1]{\mathsf{#1}}
\newcommand{\mi}[1]{\mathit{#1}}
\newcommand{\U}{\mathsf{U}}
\newcommand{\Code}{\mathsf{Code}}
\newcommand{\Ty}{\mathsf{Ty}}
\newcommand{\Bool}{\msf{Bool}}
\newcommand{\true}{\msf{true}}
\newcommand{\id}{\msf{id}}
\newcommand{\qtm}[1]{\langle #1\rangle}

%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%  PLEASE, ONLY IN ENGLISH                                         %
%  PLEASE, USE LATEX2E                                             %

%                                                                  %

%  PLEASE, DO NOT DEFINE NEW COMMANDS                              %

%  USE NEWTHEOREMS BELOW, PLEASE, IF YOU WRITE THEOREMS AND SUCH   %

%                                                                  %

%  PLEASE, AT MOST TWO PAGES                                       %

%  PLEASE, CHECK THE NUMBER OF PAGES BEFORE SUBMISSION             %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%

\newtheorem{theo}{Theorem}
\newtheorem{defi}[theo]{Definition}
\newtheorem{axio}[theo]{Axiom}
\newtheorem{clai}[theo]{Claim}
\newtheorem{coll}[theo]{Collorary}
\newtheorem{lemm}[theo]{Lemma}
\newtheorem{conj}[theo]{Conjecture}
\newtheorem{hypo}[theo]{Hypothesis}
\newtheorem{rema}[theo]{Remark}
\newcommand{\proof}{\noindent{\bf Proof:}\hspace{0.2cm}}

\setcounter{theo}{0}

%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textwidth 15.0cm
\textheight 22.0cm
\oddsidemargin 0.4cm
\evensidemargin 0.4cm
\topmargin  0.0cm
\frenchspacing

\pagestyle{myheadings}

\markboth{Developments in Computer Science,%
June 17 -- 19, 2021, ELTE,  Hungary}
{Developments in Computer Science,
June 17 -- 19, 2021, ELTE, Hungary}
%
%
\begin{document}
%
%
\begin{center}{\Large\bf
Staged Compilation and Generativity
}\end{center}
%
\begin{center}{\large\bf\noindent
András Kovács
}\\[2mm]
Eötvös Loránd University, Department of Programming Languages and Compilers
\\[1mm]\texttt{
kovacsandras@inf.elte.hu
}\end{center}
%
%
\vspace*{7mm}
%
%

\subsection*{Overview}

The purpose of staged compilation is to write code-generating programs in a safe
and ergonomic way. Although it is always possible to write metaprograms by
simply manipulating strings or deeply embedded syntax trees, this is often
error-prone and tedious. Staging is a way to have more guarantees about the
safety and well-typing of metaprograms, and also a way to integrate object-level
and meta-level syntaxes more organically.

Two-level type theory (2LTT) \cite{twolevel} was originally developed for the
purpose of doing synthetic homotopy theory, by adding a metaprogramming layer on
top of homotopy type theory \cite{hottbook}. However, it turns out that 2LTT is
also a great framework for metaprogramming and staging in general, and it is
applicable to a wide range of theories, both on the object and meta level.

There is a simple semantics to 2LTT which justifies the metaprogramming view:
this is the \emph{presheaf model} of 2LTT. Here, meta-level types are presheaves
over the underlying category of the object theory. Hence, every meta-level
construction must be stable under the object-level morphisms. The advantage of
working in 2LTT stems from stability under morphisms: if every construction is
automatically stable, it becomes possible to omit explicit handling of base
morphisms. More concretely, from the staging perspective, this means that we
never have to deal with scoping, renaming, substitution or de Bruijn indices in
the object syntax, when working in 2LTT.
\\\\
\textbf{Generativity.} Generativity means that we can only generate code, but
not make decisions based on the internal structure of object-level
syntax. Generativity simplifies staging, and it is often enforced in practical
implementations \cite{kiselyov14metaocaml}. However, non-generative staging
provides additional power and flexibility. A simple example for a non-generative
feature is conversion checking. This can be viewed as an axiom in 2LTT, which
says that meta-level (``strict'') equality of object-level values is decidable.
We aim to investigate semantics of non-generativity in the following.

\subsection*{Basic Rules and Usage of 2LTT}

To illustrate using 2LTT for staging, we specify a simple variant of 2LTT where
we have exactly the same dependent type theory for the object-level and
meta-level theories.

We have universes $\U^s_{i}$, where $s \in \{0,1\}$, denoting a \emph{stage} or
level in the 2LTT sense, and $i \in \mathbb{N}$ denotes a usual level index of
\emph{sizing} hierarchies. The two dimensions of indexing are orthogonal, and we
will elide the $i$ indices in the following. We assume Russell-style universes.
Both $\U_0$ and $\U_1$ may be closed under arbitrary type formers, but
eliminators in each \emph{only target the same universe}, i.e.\ elimination
cannot cross universes. We have the following operations:
\begin{itemize}
\item
  For $A : \U_0$, we have $\Code\,A : \U_1$. This is the type of meta-level
  programs which return object-level code with type $A$.
\item
  Quoting: for $A : \U_0$ and $t : A$ we have $\qtm{t} : \Code\,A$. In other
  words, for every object-level term we have a metaprogram which immediately
  returns that term.
\item Splicing: for $t : \Code\,A$, we have $\sim\!t : A$. This means running
  a metaprogram and inserting its result into object-level code.
\item We also know that quoting/splicing is a definitional isomorphism, so
  $\qtm{\sim\!t} = t$ and  $\sim\!\qtm{t} = t$.
\end{itemize}

A \emph{staging algorithm} takes as input a closed term $t : A$ where $A :
\U_0$, and splices the results of all metaprograms, so that we get an output
term which is free of splices. This can be implemented using variations of
normalization-by-evaluation \cite{abel2013normalization} which track current
stages. We do not detail staging algorithms here.

Let's look at some examples. We have the object-level identity function as usual:
\begin{alignat*}{3}
  &\msf{id_0} : (A : \U_0) \to A \to A\\
  &\msf{id_0} := \lambda\,A\,x.\,x
\end{alignat*}
Staging does not do anything with $\msf{id_0}$, since it has no
splices. Likewise if we apply $\msf{id_0}$ to object-level values, as in $\msf{id_0}\,\Bool\,\msf{true}$. We also have the meta-level version:
\begin{alignat*}{3}
  &\msf{id_1} : (A : \U_1) \to A \to A\\
  &\msf{id_1} := \lambda\,A\,x.\,x
\end{alignat*}
Note that this also works on object-level values, because of quoting:
\[
   \sim\!(\msf{id_0}\,(\Code\,\Bool_0)\,\qtm{\true_0}) : \Bool_0
\]
Staging the above term computes to $\sim\!\qtm{\true_0}$, which in turn
computes to $\true_0$. Thus, $\id_1$ is compile-time evaluated. There's a third
version, which is a specialized version of $\id_1$: it's also evaluated at compile time,
but it only works on object-level types:
\begin{alignat*}{3}
  &\msf{id_{Code}} : (A : \Code\,\U_0) \to \Code\,(\sim\!A) \to \Code\,(\sim\!A)\\
  &\msf{id_{Code}} := \lambda\,A\,x.\,x
\end{alignat*}
Now, $\sim\!(\id_{\Code}\,\qtm{\Bool_0}\,\qtm{\true_0})$ also stages to
$\true_0$. Meta-functions which are restricted to $\Code$ are also useful when
we want to define functions which are partially evaluated at compile time. For
example, if we want to inline a function argument for object-level list mapping:
\begin{alignat*}{3}
  &\rlap{$\msf{map} : (A\,B : \Code\,\U_0) \to (\Code(\sim\!A) \to \Code(\sim\!B))$}\\
  &\hspace{2em}\rlap{$\to \Code(\msf{List_0}\,(\sim\!A)) \to \Code(\msf{List_0}\,(\sim\!B)))$}\\
  &\msf{map} := \lambda\,A\,B\,f\,\mi{as}.\,\qtm{\msf{foldr_0}\,
    (\lambda\,a\,bs.\,\msf{cons_0}\,(\sim\!(f\,\qtm{a}))\,bs)\,\msf{nil_0}\,(\sim\!as)}
\end{alignat*}

\subsection*{Presheaf Model}

Why consider the presheaf model?  The reason is that it's the simplest semantics
which justifies the metaprogramming view of 2LTT. It is \emph{not} the same
thing as the staging algorithm, which is based on normalization-by-evaluation,
which is much more complicated to formalize \cite{bocquet2021induction}. The
presheaf model is fairly simple as far as models go, so it's interesting to see
which staging features can be justified with it. We skip presenting the whole
presheaf model. For details, we refer the reader to
\cite[Section~1.2]{huber-thesis}.

We give some examples for interpreting constructions in the model. We present the
results up to isomorphism, with some simplifications. We assume now that
object-level morphisms are substitutions. We have $\Bool_0 : \U_0$ and $\Bool_1
: \U_1$.
\begin{itemize}
  \item A closed function $t : \Bool_1 \to \Bool_1$ becomes a metatheoretical
    function in $\mathbb{B} \to \mathbb{B}$.
  \item A closed function $t : \Bool_0 \to \Bool_0$ becomes a closed object-theoretic
    function in $\Bool \to \Bool$.
  \item A closed function $t : \Code\,\Bool_0 \to \Bool_1$ becomes a function
    which maps a $\Bool$ term in any context to $\mathbb{B}$, such that the
    function commutes with object-theoretic substitution. For example if we have
    a variable $x$, we can substitute it with any term before feeding it to the
    semantic $t$, and the result is the same. In fact, this means that $t$
    cannot depend on its term argument, hence $t$ is specified simply by a
    $\mathbb{B}$.
  \item
    A closed function $t : \Bool_1 \to \Code\,\Bool_0$ becomes simply a pair of
    closed $\Bool$ terms.
\end{itemize}
\noindent\textbf{Yoneda lemma.} The Yoneda lemma is a general statement which
restricts the way meta-level values can depend on object-level ones. First, note that any
object-level typing context $\Gamma$ can be mapped to a presheaf, by taking the
sets of parallel substitutions into $\Gamma$. This is the \emph{Yoneda
embedding} of $\Gamma$, denoted by $\msf{y}\Gamma$. The Yoneda lemma says
that we have the following isomorphism of sets:
\[
   (\msf{y}\,\Gamma \Rightarrow \Delta) \simeq |\Delta|\,\Gamma
\]
where $\Rightarrow$ means a natural transformation, and $|\Delta|\,\Gamma$
denotes the set that we get by evaluating the $\Delta$ presheaf at the
object-level $\Gamma$ context. From this, what we essentially get is that any
2LTT term $\Gamma \vdash t : A$, such that $\Gamma$ is essentially interpreted
as $\msf{y}\,\Gamma'$ for some $\Gamma'$, is interpreted as an element of
$|A|\Gamma'$. We call $\Gamma$ \emph{representable} if there is such $\Gamma'$.

In particular, if $\Gamma \vdash t : \Bool_1$ and $\Gamma$ representable, then
since $|\Bool_1|\,\Gamma' = \mathbb{B}$, $t$ is simply an element of
$\mathbb{B}$ in the semantics, and cannot depend on the typing context.

In short, whether the Yoneda lemma applies to a given term, depends on whether
the typing context is representable. In turn, the representability of the context
depends on what morphisms are in the object theory. We consider two options.
\\\\
\textbf{1. Morphisms are substitutions.} In this case, $\msf{y}$ preserves
context extension, i.e.\ $\msf{y}\,(\Gamma,\,x : A) \simeq (\msf{y}\,\Gamma,\,x
: \msf{y}A)$ in the presheaf model. That's because a substitution which targets
$(\Gamma,\,x : A)$ is equivalent to a pair of substitutions, targeting $\Gamma$
and $A$ respectively. Therefore, if we have $x_1 : A_1,\,x_2 : A_2,\,...,\, x_i :
A_i \vdash t : B$, such that all $A_i$ are representable, the entire context is
also representable, and the Yoneda lemma applies.

Which types are representable? For starters, every type of the form $\Code\,A$,
since $\Code$ in the model is essentially interpreted as $\msf{y}$ (eliding the
formal complications arising from possible dependencies of $A$ on the context).
Also, if we have $x : A$ in a context, where $A : \U_0$, that context extension
is also interpreted as extension with $\msf{y}A$. In short: if the context
only has $\Code$ types or types in $\U_0$, it is representable.

This greatly limits non-generative features in the model. Consider adding the
axiom which says that meta-level equality of object-level values is decidable:
$x : \Code\,A,\,y : \Code\,A \vdash \msf{conversion}_A : (x = y) + (x = y \to
\bot)$.  This is a simple non-generative axiom, since any constructive
interpretation must look inside $\Code$-s. This axiom is false if object
morphisms are substitutions. That's because the context is representable, so we
can simplify using the Yoneda lemma. The statement that we get in the model is
that ``two terms are either equal, or they are inequal and remain inequal after
arbitrarily substitutions''. Now, if we pick two \emph{variables} $x$ and $y$
such that $x \neq y$, then they are not equal, but they can be also made
equal by substituting both variables with the same term.

Can we repair this? One possibility is to have decidable equality only for
\emph{closed} terms. However, the syntax of 2LTT provides no way to talk about
closed terms. Instead we'd have to use a closed modality
\cite{bocquet2021induction}. This would be interesting to investigate in future
work.
\\\\
\textbf{2. Morphisms are weakenings.} In this case, object morphisms are are
so-called \emph{order-preserving embeddings}, meaning that a morphism can drop
zero or more entries from a context, so morphisms are essentially bitmasks which
mark a sub-context. The action of weakening embeds terms in larger
contexts. Moreover, $\msf{y}$ does not preserve context extension. Hence,
typing contexts are not necessarily representable, even if they only contain object-level
bindings. So the Yoneda-reduction of dependencies generally does not apply.

Now, $\msf{conversion}$ is fine, because inequality of terms is stable under
weakening. On the other hand, by only having weakening in the object theory, the
range of supported object theories is greatly restricted. For example, we can't
have $\beta$-reduction for functions in the equational theory, since that's
specified using substitution. Likewise, dependent types are out, since the
typing of dependent elimination involves substitution.

Simple type theories still work, if we only have weakening in their equational
theory. For the perspective of staging, this is fine, because in \emph{code
generation} we care about the intensional definition of programs, and we do not
want to equate $\beta$-reducts, since a primary use-case of staging is to
improve runtime performance, hence distinguish between possibly
$\beta$-convertible programs.

It appears that if morphisms are weakenings, then the presheaf model is
compatible with a wide range of non-generative axioms. For example, we can also
postulate \emph{countability} of $\Code\,A$, i.e.\ that there are injections
$\msf{index}_A : \Code\,A \to \msf{Nat}_1$. In the presheaf model, the indexing function
works by enumerating maximally strengthened terms, which are stable under
weakening.
\\\\
\textbf{Other ways of justifying non-generativity.} An alternative solution
would be to use something other than the presheaf model to justify
non-generative axioms. For example: could we use the staging algorithm itself,
i.e.\ does normalization-by-evaluation support non-generativity? It seems
likely, as semantic values need only be stable under weakening. This remains
future work.

\begin{thebibliography}{1}

\bibitem{abel2013normalization}
Andreas Abel.
\newblock {\em Normalization by Evaluation: Dependent Types and
  Impredicativity}.
\newblock PhD thesis, Ludwig-Maximilians-Universit{\"a}t M{\"u}nchen, 2013.
\newblock Habilitation thesis.

\bibitem{twolevel}
Danil Annenkov, Paolo Capriotti, Nicolai Kraus, and Christian Sattler.
\newblock Two-level type theory and applications.
\newblock {\em ArXiv e-prints}, may 2019.

\bibitem{bocquet2021induction}
Rafa{\"e}l Bocquet, Ambrus Kaposi, and Christian Sattler.
\newblock Induction principles for type theories, internally to presheaf
  categories.
\newblock {\em arXiv preprint arXiv:2102.11649}, 2021.

\bibitem{huber-thesis}
Simon Huber.
\newblock {\em Cubical Interpretations of Type Theory}.
\newblock PhD thesis, University of Gothenburg, 2016.

\bibitem{kiselyov14metaocaml}
Oleg Kiselyov.
\newblock The design and implementation of {BER} metaocaml - system
  description.
\newblock In Michael Codish and Eijiro Sumii, editors, {\em Functional and
  Logic Programming - 12th International Symposium, {FLOPS} 2014, Kanazawa,
  Japan, June 4-6, 2014. Proceedings}, volume 8475 of {\em Lecture Notes in
  Computer Science}, pages 86--102. Springer, 2014.

\bibitem{hottbook}
The {Univalent Foundations Program}.
\newblock {\em Homotopy Type Theory: Univalent Foundations of Mathematics}.
\newblock \url{https://homotopytypetheory.org/book}, Institute for Advanced
  Study, 2013.
\end{thebibliography}

%% --------------------------------------------------------------------------------

\let\msf\undefined
\let\mi\undefined
\let\U\undefined
\let\Code\undefined
\let\Ty\undefined
\let\Bool\undefined
\let\true\undefined
\let\id\undefined
\let\qtm\undefined

%% --------------------------------------------------------------------------------

\end{document}
