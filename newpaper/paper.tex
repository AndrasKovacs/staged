
%% build: latexmk -pdf -pvc paper.tex

\documentclass[acmsmall,screen,review,anonymous]{acmart}
%% \documentclass[nonacm,acmsmall,review]{acmart}
%% \raggedbottom

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


%% \setcopyright{rightsretained}
%% \acmPrice{}
%% \acmDOI{10.1145/3547641}
%% \acmYear{2022}
%% \copyrightyear{2022}
%% \acmSubmissionID{icfp22main-p52-p}
%% \acmJournal{PACMPL}
%% \acmVolume{6}
%% \acmNumber{ICFP}
%% \acmArticle{110}
%% \acmMonth{8}

%% These commands are for a JOURNAL article.

%% \acmJournal{JACM}
%% \acmVolume{37}
%% \acmNumber{4}
%% \acmArticle{111}
%% \acmMonth{8}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

\bibliographystyle{ACM-Reference-Format}
%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
\citestyle{acmauthoryear}

%% --------------------------------------------------------------------------------

\usepackage{xcolor}
\usepackage{mathpartir}
\usepackage{todonotes}
\presetkeys{todonotes}{inline}{}
\usepackage{scalerel}
\usepackage{bm}
\usepackage{mathtools}
%% \usepackage{amssymb}


\newcommand{\mit}[1]{\mathit{#1}}
\newcommand{\msf}[1]{\mathsf{#1}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\mdo}{\mbf{do}\,}
\newcommand{\ind}{\hspace{1em}}
\newcommand{\bif}{\mbf{if}\,}
\newcommand{\bthen}{\mbf{then}\,}
\newcommand{\belse}{\mbf{else}\,}
\newcommand{\return}{\mbf{return}\,}
\newcommand{\lam}{\lambda\,}
\newcommand{\data}{\mbf{data}\,}
\newcommand{\where}{\mbf{where}}
\newcommand{\M}{\msf{M}}
\newcommand{\letrec}{\mbf{letrec}\,}
\newcommand{\of}{\mbf{of}\,}
\newcommand{\go}{\mit{go}}
\newcommand{\add}{\mit{add}}
\newcommand{\letdef}{\mbf{let\,}}
\newcommand{\map}{\mit{map}}
\newcommand{\vas}{\mit{as}}
\newcommand{\vbs}{\mit{bs}}
\newcommand{\vxs}{\mit{xs}}
\newcommand{\vys}{\mit{ys}}
\newcommand{\vma}{\mit{ma}}
\newcommand{\vmb}{\mit{mb}}

\newcommand{\ext}{\triangleright}

\newcommand{\Int}{\msf{Int}}
\newcommand{\List}{\msf{List}}
\newcommand{\Nil}{\msf{Nil}}
\newcommand{\Cons}{\msf{Cons}}
\newcommand{\Reader}{\msf{Reader}}
\newcommand{\ReaderT}{\msf{ReaderT}}
\newcommand{\Monad}{\msf{Monad}}
\newcommand{\Applicative}{\msf{Monad}}
\newcommand{\class}{\msf{class}}
\newcommand{\Functor}{\msf{Functor}}
\newcommand{\Bool}{\msf{Bool}}
\newcommand{\Statel}{\msf{State}}
\newcommand{\fro}{\leftarrow}
\newcommand{\case}{\mbf{case\,}}

\newcommand{\Lift}{{\Uparrow}}
\newcommand{\Up}{{\Uparrow}}
\newcommand{\spl}{{\bs{\sim}}}
\newcommand{\ql}{{\bs{\langle}}}
\newcommand{\qr}{{\bs{\rangle}}}
\newcommand{\bind}{\mathbin{>\!\!>\mkern-6.7mu=}}

\newcommand{\MTy}{\msf{MetaTy}}
\newcommand{\VTy}{\msf{ValTy}}
\newcommand{\Ty}{\msf{Ty}}
\newcommand{\CTy}{\msf{CompTy}}
\newcommand{\True}{\msf{True}}
\newcommand{\False}{\msf{False}}
\newcommand{\fst}{\msf{fst}}
\newcommand{\snd}{\msf{snd}}

\newcommand{\blank}{{\mathord{\hspace{1pt}\text{--}\hspace{1pt}}}}

\newcommand{\Nat}{\msf{Nat}}
\newcommand{\Zero}{\msf{Zero}}
\newcommand{\Suc}{\msf{Suc}}
\newcommand{\Maybe}{\msf{Maybe}}
\newcommand{\MaybeT}{\msf{MaybeT}}
\newcommand{\Nothing}{\msf{Nothing}}
\newcommand{\Just}{\msf{Just}}

\theoremstyle{remark}
\newtheorem{notation}{Notation}
\newcommand{\id}{\mit{id}}
\newcommand{\mup}{\mit{up}}
\newcommand{\mdown}{\mit{down}}
\newcommand{\tyclass}{\mbf{class}}
\newcommand{\instance}{\mbf{instance}\,}
\newcommand{\Improve}{\msf{Improve}}
\newcommand{\Gen}{\msf{Gen}}
\newcommand{\unGen}{\mit{unGen}}
\renewcommand{\Vec}{\msf{Vec}}
\newcommand{\gen}{\mit{gen}}
\newcommand{\genRec}{\mit{genRec}}
\newcommand{\fmap}{<\!\!\$\!\!>}
\newcommand{\ap}{<\!\!*\!\!>}
\newcommand{\runGen}{\mit{runGen}}
\newcommand{\qt}[1]{\ql#1\qr}
\newcommand{\lift}{\mit{lift}}
\newcommand{\liftGen}{\mit{liftGen}}
\newcommand{\MonadGen}{\msf{MonadGen}}
\newcommand{\MonadState}{\msf{MonadState}}
\newcommand{\MonadReader}{\msf{MonadReader}}
\newcommand{\RA}{\Rightarrow}
\newcommand{\EitherT}{\msf{EitherT}}
\newcommand{\StateT}{\msf{StateT}}
\newcommand{\Identity}{\msf{Identity}}

\newcommand{\Stop}{\msf{Stop}}
\newcommand{\Skip}{\msf{Skip}}
\newcommand{\Yield}{\msf{Yield}}

\newcommand{\runIdentity}{\mit{runIdentity}}
\newcommand{\runReaderT}{\mit{runReaderT}}
\newcommand{\newtype}{\mbf{newtype}\,}
\newcommand{\runMaybeT}{\mit{runMaybeT}}
\newcommand{\runStateT}{\mit{runStateT}}
\newcommand{\runState}{\mit{runState}}
\newcommand{\dlr}{\,\$\,}
\newcommand{\ImproveF}{\msf{ImproveF}}
\newcommand{\ExceptT}{\msf{ExceptT}}
\newcommand{\State}{\msf{State}}
\newcommand{\SumVS}{\msf{SumVS}}
\newcommand{\ProdCS}{\msf{ProdCS}}
\newcommand{\Here}{\msf{Here}}
\newcommand{\There}{\msf{There}}
\newcommand{\IsSumVS}{\msf{IsSumVS}}
\newcommand{\MonadJoin}{\msf{MonadJoin}}
\newcommand{\Stream}{\msf{Stream}}
\newcommand{\join}{\mit{join}}
\newcommand{\modify}{\mit{modify}}
\newcommand{\get}{\mit{get}}
\newcommand{\mput}{\mit{put}}
\newcommand{\mAs}{\mit{As}}
\newcommand{\Rep}{\mit{Rep}}
\newcommand{\encode}{\mit{encode}}
\newcommand{\decode}{\mit{decode}}
\newcommand{\mindex}{\mit{index}}
\newcommand{\mtabulate}{\mit{tabulate}}
\newcommand{\States}{\mit{States}}
\newcommand{\seed}{\mit{seed}}
\newcommand{\step}{\mit{step}}
\newcommand{\Step}{\msf{Step}}
\newcommand{\Pull}{\msf{Pull}}
\newcommand{\MkPull}{\msf{MkPull}}

%% --------------------------------------------------------------------------------

%%
%% end of the preamble, start of the body of the document source.
%% \hypersetup{draft}
\begin{document}

\title{Closure-Free Functional Programming in a Two-Level Type Theory}
%% \titlenote{}

%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{András Kovács}
\email{andrask@chalmers.se}
\orcid{0000-0002-6375-9781}
\affiliation{%
  \institution{University of Gothenburg}
  \country{Sweden}
  \city{Gothenburg}
}

\begin{abstract}
There are many abstraction tools in modern functional programming which heavily
rely on general-purpose compiler optimization to achieve adequate
performance. For example, monadic binding is a higher-order function which
yields runtime closures in the absence of sufficient compile-time inlining and
beta-reductions, thereby significantly degrading performance. In current systems
such as the Glasgow Haskell Compiler, there is no strong guarantee that
general-purpose optimization can eliminate abstraction overheads, and users only
have indirect and fragile control over code generation through inlining
directives and compiler options. In this paper we propose using a two-stage
language to simultaneously get strong code generation guarantees and strong
abstraction features. The object language is a simply typed first-order language
where all function calls are statically known, and which can be compiled without
runtime closures. The compile-time language is a dependent type theory. The two
are integrated in a two-level type theory. We develop some abstraction
tools in this setting. First, we develop monads and monad transformers. Second,
we develop fusion for push and pull streams. Most of our results are also
adapted to a proof-of-concept library in typed Template Haskell.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003752.10003790.10011740</concept_id>
       <concept_desc>Theory of computation~Type theory</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011041.10011047</concept_id>
       <concept_desc>Software and its engineering~Source code generation</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Theory of computation~Type theory}
\ccsdesc[500]{Software and its engineering~Source code generation}

\keywords{two-level type theory, staged compilation}

\maketitle


\section{Introduction}\label{sec:introduction}

Modern functional programming supports many convenient abstractions. These often
come with significant runtime overheads. Sometimes the overheads are acceptable,
but in other cases compiler optimization is crucial. Monads in Haskell is an
example for the latter. Even the $\Reader$ monad, which is one of the simplest
in terms of implementation, yields large overheads when compiled without
optimizations. Consider the following snippet.
\begin{alignat*}{3}
  &f :: \Int \to \Reader\,\Bool\,\Int \\
  &f\,x = \mdo\{b \fro \msf{ask}; \bif x\, \bthen \return (x + 10)\, \belse \return (x + 20)\}
\end{alignat*}
With optimizations enabled, GHC compiles this roughly to the following:
\begin{alignat*}{3}
  &f :: \Int \to \Bool \to \Int \\
  &f = \lam x\,b.\, \bif b\, \bthen x + 10\, \belse x + 20
\end{alignat*}
Without optimizations we roughly get:
\begin{alignat*}{3}
  &f = \lam x.\,(\bind)\,\msf{MonadReaderDict}\,\msf{ask}\,(\lam b.\,\bif b\\
  &\ind \bthen\return\,\msf{MonadReaderDict}\,(x + 10)\\
  &\ind \belse\hspace{0.25em}\return\,\msf{MonadReaderDict}\,(x + 20))
\end{alignat*}
Here, $\msf{MonadReaderDict}$ is a runtime dictionary, containing the methods of
the $\Monad$ instance for $\Reader$, and $(\bind)\,\msf{MonadReaderDict}$ is a
field projection. Here, a runtime closure will be created for the $\lam b.\,...$
function, and $(\bind)$, $\msf{ask}$ and $\return$ will create additional dynamic
closures.

The difference between optimized and unoptimized code is already large here, and
it gets even larger when we consider monad transformers or code that is
polymorphic over monads. In Haskell, such code is pervasive, even in fairly
basic programs which do not use fancy abstractions. The $\msf{mapM}$ function in
the Haskell Prelude, which maps over a list in some monad, is a third-order and
second-rank polymorphic function in disguise, because its monad dictionary
argument contains the polymorphic second-order $(\bind)$ method.
\begin{alignat*}{3}
  & \msf{mapM} :: \Monad\,m \Rightarrow (a \to m\,b) \to [a] \to m\,[b]
\end{alignat*}
Compiling $\msf{mapM}$ efficiently relies on inlining the instance dictionary,
then inlining the methods contained there, and also inlining the functions
that the higher-order binding method is applied to.

\todo{REORG intro, trim, contrib, motivate monadtrans \& stream fusion}

GHC's optimization efforts are respectable, and it has gotten quite good over
its long history of development. However, there is no strong guarantee that
certain optimizations will happen. Control over optimizations remains tricky,
fragile and non-compositional. \texttt{INLINE} and \texttt{REWRITE} pragmas can
be used to control code generation, but without any strong guarantee, and their
sophisticated usage requires knowledge of GHC internals. For example, correctly
specifying the \emph{ordering} of certain rule applications is often needed. We
have to also care about function arities. Infamously, the function composition
operator is defined as $(.)\,f\,g = \lam x \to f\,(g\,x)$ in the base libraries,
instead of as $(.)\,f\,g\,x = f\,(g\,x)$, to get better inlining behavior ---
as explained in a source comment right next to the definition \cite{TODO}.

Although there are numerous tricks and idioms that are used in high-performance
Haskell programming, for reliable performance it is necessary that programmers
periodically \emph{review} GHC's optimized code output. Needless to say, this is
rather time-consuming and poorly scalable.

\subsection{Staged Compilation}

In this paper we use staged compilation to address issues of robustness. The
idea is to shift as much as possible work from general-purpose optimization to
metaprograms.

Metaprograms can be deterministic, transparent, and can be run efficiently,
using fast interpreters or machine code compilation. In contrast,
general-purpose optimizers are slower to run, less transparent and less
reliable. Also, metaprogramming allows library authors to exploit
domain-specific optimizations, while it is not realistic for general-purpose
optimizers to know about all domains.

On the other hand, metaprogramming requires some additional care and input from
programmers. Historically, there have been problems with ergonomics as well:
\begin{itemize}
\item Code generation might fail \emph{too late} in the pipeline, producing
      incomprehensible errors; this is often caused by not having enough static
      guarantees about the well-formedness of code output.
\item Tooling for the object language (debugging, profiling, IDE support) often does not
      work for metaprogramming. This is more likely if the object and meta layers are wildly
      different.
\item Metaprogramming may introduce heavy noise and boilerplate, obscuring the logic of
      programs, or impose restrictions on how code can be structured. For instance,
      Template Haskell mandates that metaprograms used in splices in some module are
      defined in a different module.
\end{itemize}

The idea of \textbf{two-level type theory} (2LTT) is to use a highly expressive
dependent type theory for compile-time computation, but generate code in
possibly different, simpler object language. In this paper, we use 2LTT to sweeten the
deal of staged compilation, aiming for a combination of strong guarantees, good
ergonomics, high level of abstraction and easy-to-optimize code output.

We develop a particular two-level type theory for this purpose, which we
call \textbf{CFTT}, short for ``closure-free type theory''. This consists
of
\begin{itemize}
\item A simply-typed object theory with first-order functions, general recursion and
      finitary algebraic data types. This language is easy to optimize and compile
      downstream in the pipeline, but it lacks many convenience features.
\item A standard Martin-Löf type theory for the compile-time language. This
      allows us to recover many features by metaprogramming.
\end{itemize}

In particular, since the object language is first-order, we guarantee that all
programs in CFTT can be ultimately compiled without any dynamic closures, using
only calls and jumps to statically known code locations. Why focus on closures?
They are the foundation to almost all abstraction tools in functional
programming:
\begin{itemize}
\item Higher-order functions in essentially all functional languages are implemented with closures.
\item Type classes in Haskell use dictionary-passing, which relies on closures for function methods.
\item Functors and first-class modules in OCaml and other ML-s rely on closures.
\end{itemize}
Hence, doing functional programming without closures is a clear demonstration
that we can get rid of abstraction overheads.

It turns out that surprisingly little practical programming relies essentially
on closures. Most of the time, programmers use higher-order functions for
\emph{abstraction}, such as when mapping over lists, where it is expected that
the mapping function will be inlined. In other cases, the use of closures can be
eliminated by small-scale defunctionalization. For example, difference lists
\cite{TODO} are often implemented with closures, but they can be also
implemented as binary trees, with the same programming interface.

\todo{Essential closures: CPS? Threaded interpreters?}

Whole-program defunctionalization is also possible, and it is notably used by
the MLton compiler \cite{TODO}. While this can be practical and effective, it
can be also expensive and require whole-program processing. Also, conceptually
speaking, it does not eliminate closures but instead makes them more transparent
to optimizations. In this paper we do not use defunctionalization.

We note though that our setup is compatible with closures as well, and it can
support two separate type formers for closure-based and non-closure-based
(``static'') functions. Having both of these would be desirable in a practical
system. In the current work we focus on the closure-free case because it is much
less known and developed, and it is quite interesting to see how far we
can go with it.

\subsection{Contributions}

\todo{CONTRIBUTIONS}


\section{Basics of CFTT}\label{sec:basics-of-cftt}

In the following we give an overview of CFTT features. Here we focus on examples
and informal explanations. We defer the more formal details to Sections
\ref{TODO} and \ref{TODO}. We first review the meta-level language, then the
object-level one, and finally the staging operations which bridge between the
two.

\subsection{The Meta Level}\label{sec:the-meta-level}

$\bs{\MTy}$ is the universe of types in the compile-time language. We will often
use the term ``metatype'' to refer to inhabitants of $\MTy$, and use
``metaprogram'' for inhabitants of metatypes. $\MTy$ supports dependent
functions, products and indexed inductive types \cite{TODO}.

Formally, $\MTy$ is additionally indexed by universes levels, and we have
$\MTy_i : \MTy_{i+1}$. However, this adds a bit of noise, and it is not very
relevant to the current paper, so we shall omit levels. Note that universe
levels have nothing to do with staged compilation; they are about sizing for the
purpose of logical consistency.

\todo{If we won't use Sigma notation, get rid of it}

We use Agda-like syntax for functions and implicit arguments. A basic example:
\begin{alignat*}{3}
  &\id : \{A : \MTy\} \to A \to A\\
  &\id = \lam x.\, x
\end{alignat*}
Here, the type argument is implicit, and it gets inferred when we use the
function. For example, $id\,\True$ is elaborated to $id\,\{\Bool\}\,\True$,
where the braces mark an explicit application for the implicit argument.
Similarly, we can introduce implicit lambdas explicitly:
\begin{alignat*}{3}
  &id = \lam \{A : \MTy\}(x : A).\,x
\end{alignat*}
We write $(a : A) \times B$ for $\Sigma$-types in $\MTy$. For the field
projections, for $t : (a : A) \times B$, we have $\fst\,t : A$ and $\snd\,t :
B[a \mapsto \fst\,t]$, and pairing is written simply as $(t,\,u)$. Additionally,
we use syntactic sugar for field projections: we can write a type as $(field_1 :
A) \times (field_2 : B) \times (field_3 : C)$, and write $t.field_2$ for a named
field projection. The type itself denotes a right-nested $\Sigma$-type. We can
also write $(t,\,u,\,v)$ for a right-nested iterated pairing. In Haskell style,
we write $()$ for the unit type, and also for its inhabitant.

Inductive types can be introduced using a Haskell-like ADT notation, or with a GADT-style one:
\begin{alignat*}{3}
  &                                           &&\hspace{4em}\data\,\Bool_\M &&: \MTy\,\where\\
  & \data\,\Bool_\M : \MTy = \True_\M\,|\,\False_\M &&\hspace{4em}\ind\ind \True_\M &&: \Bool_\M\\
  &                                           &&\hspace{4em}\ind\ind \False_\M &&: \Bool_\M
\end{alignat*}
Note that we added an $_\M$ subscript to the type; when analogous types can be
defined both on the meta and object levels, we will sometimes use this subscript
to disambiguate the meta-level version. We will also use Haskell-like newtype
notation, such as in $\newtype \msf{Wrap}\,A = \msf{Wrap}\,\{\mit{unWrap} : A
\}$.

Importantly, we can only produce meta-level values by induction on meta-level
data. In general, all construction and elimination rules for type formers in
$\MTy$ stay within $\MTy$.

\subsection{The Object Level}\label{sec:the-object-level}

$\bs{\Ty}$ is the universe of types in the object language. It is itself a
metatype, so so we have $\Ty : \MTy$. Similarly as in the case of $\MTy$, all
construction and elimination rules of the object language stay within $\Ty$.
However, we further split $\Ty$ to two sub-universes.

First, $\bs{\VTy} : \MTy$ is the universe of \emph{value types}. $\VTy$ supports
parameterized algebraic data types, where parameters can have arbitrary types,
but all constructor field types must be in $\VTy$.

Since $\VTy$ is a sub-universe of $\Ty$, we have that when $A : \VTy$ then also
$A : \Ty$. Formally, this is specified as an explicit embedding operation but
informally it is nicer to have an implicit subtyping.

Second, $\bs{\CTy} : \MTy$ is the universe of \emph{computation types}. This is
also a sub-universe of $\Ty$ with implicit coercions. For now, we only specify
that $\CTy$ contains functions whose domains are value types:
\[ \blank\to\blank : \VTy \to \Ty \to \CTy \]
For instance, if $\Bool : \VTy$ is defined as an object-level ADT, then $\Bool
\to \Bool : \CTy$, hence also $\Bool \to \Bool : \Ty$. However, $(\Bool \to
\Bool) \to \Bool$ is ill-formed, since the domain is not a value type. Let us look at an
example for an object-level program, where we already have natural numbers
declared as $\data \Nat := \Zero\,|\,\Suc\,\Nat$:
\begin{alignat*}{3}
  &\hspace{-1em}\rlap{$\add : \Nat \to \Nat \to \Nat$}\\
  &\hspace{-1em}\add :=\,&& \mathrlap{\letrec \go\,n\,m := \case n\,\of}\\
  &\hspace{-1em}         && \ind \Zero   &&\to m;\\
  &\hspace{-1em}         && \ind \Suc\,n &&\to \Suc\,(\go\,n\,m);\\
  &\hspace{-1em}         && \go
\end{alignat*}
Every recursive definition must be introduced with $\letrec$. The general syntax
is $\letrec x : A := t; u$, where the $A$ type annotation can be
omitted. $\mbf{letrec}$ can be only used to define computations, not values
(hence, only functions can be recursive so far).

Object-level definitions use $:=$ as notation, instead of the $=$ that is used
for meta-level ones. Later on, we may use some implicit CFTT notations, but we
will always disambiguate the stages of $\mbf{let}$-s in this way. Non-recursive
$\mbf{let}$ is also allowed, and can be used to shadow binders:
\begin{alignat*}{3}
  &f : \Nat \to \Nat\\
  &f\,x := \letdef x := x + 10; \letdef x := x + 20; x * 10
\end{alignat*}
We also allow $\mbf{newtype}$ definitions, both in $\VTy$ and $\CTy$. These are
assumed to be erased at runtime. In the Haskell adaptation they are important
for aiding type class resolution, and we think that the explicit wrapping
makes many definitions more comprehensible in CFTT as well.

Values are call-by-value at runtime; they are computed eagerly in function
applications and $\mbf{let}$-s. $\mbf{let}$-definitions can be used to define
inhabitants of any type, and the type of the $\mbf{let}$ body can be also
arbitrary. Additionally, the right hand sides of $\case$ branches can also have
arbitrary types. So the following is well-formed:
\begin{alignat*}{3}
  &f : \mathrlap{\Bool \to \Nat \to \Nat}\\
  &f\,b := \case b\,\of \True \to (\lam x.\, x + 10); \False &&\to (\lam x.\, x * 10)
\end{alignat*}

What about \textbf{definitional equality} at the object level? This is a
distinct notion from the runtime semantics; object programs are embedded in
CFTT, which is dependently typed, and we need to decide definitional equalities
during type checking. The setup is simple: we have no $\beta$ or $\eta$ rules
for object programs at all, nor any rule for $\mbf{let}$-unfolding. The reasons
are the following. First, since the object language has general recursion, most
$\beta$-rules are only valid in the operational semantics up to some
restrictions, and $\beta$-conversion is not decidable to begin with. Second, in
metaprogramming we care about the size and efficiency of generated code, and
these properties are not stable under $\beta$ and $\eta$.  Hence, the sensible
choice is to do metaprogramming up to strict syntactic equality of object
programs.

Let us discuss the object language. First, notice that there is no polymorphism
or any kind of type dependency. Although we can define lists as $\data \List\,A
: \VTy := \Nil\,|\,\Cons\,A\,(\List\,A)$, the parameterization is just a
shorthand; all concrete instantiations of the type are distinct. This
monomorphism makes it easy to use different memory layouts for different types.
For example, types which look like products may be unboxed. We could also make a
distinction between boxed and unboxed sum types. We do not explore this in
detail, we just note that monomorphic types make it easy to control memory
layouts, and we believe that this is an important part of performance
optimization.

Second, there are no higher-order functions, and functions also cannot be stored
in data structures. Hence, locally defined functions can never escape their
scope, and all function calls are to functions defined in the current
scope. This makes it possible to run object programs without using dynamic
closures. This latter point might not be completely straightforward; what
about the previous $f$ function which has $\lambda$-expressions under a
$\mbf{case}$, should that require closures?

We say that it should not. We make this formal formal in Section \ref{TODO};
here we only give an intuitive explanation. In short, we choose a call-by-name
runtime semantics for functions, which means that the only way we can compute
with a function is by applying it to all arguments and extracting the resulting
value.  Hence, the only way to compute with $f$ is to apply it to \emph{two}
arguments, so $f$ is operationally equivalent to $\lam b\,x.\,\case b\,\of \True
\to x + 10; \False \to x * 10$.  In Section \ref{TODO} we show that all object
programs can be transformed to a \emph{saturated} form, where the arity of every
function call matches the number of topmost $\lambda$-binders in the definition
of the called function. Then, the local functions can be compiled either to
top-level functions by lambda-lifting, or if they are only tail-called, left in
place as join points \cite{TODO}.

Why not just make the object language less liberal, e.g.\ by disallowing
$\lambda$ under $\mbf{case}$ or $\mbf{let}$, thereby making call saturation
easier or more obvious? There is a trade-off between making the object language
more restricted, and thus easier to compile, and making \emph{metaprogramming}
more convenient. We will see that the ability to insert $\mbf{let}$-s without
restriction is very convenient in code generation, and likewise the ability to
have arbitrary object expressions in $\mbf{case}$ bodies. In this paper we go
with the most liberal object syntax, at the cost of needing more downstream
processing. The call-by-name nature of computation types requires a bit of a
change of thinking from programmers, but we believe that it is well worth to
have for the metaprogramming convenience.

On the other side of the spectrum, one might imagine generating object code in
low-level A-normal form. This is more laborious, but it could be also
interesting, because it forces us to invent abstractions for manipulating ANF in
the metalanguage. In a similar vein, Allais recently proposed metaprogramming
quantum circuits in a two-level type theory \cite{TODO}. We leave such setups
with low-level object languages to future investigation.

Finally, one might compare our object language to call-by-push-value
(CBPV). Indeed, we took inspiration from CBPV, and there are similarities, but
also differences. In both systems there is a value-computation distinction, and
values are call-by-value, and computations are call-by-name. However, our object
language allows variable binding at arbitrary types, while CBPV only supports it
at value types. In CBPV, a let-definition for a function is only possible by
first packing it up as a closure value (or: ``thunk''), which clearly does not
work for us.  Also, CBPV makes a judgment-level structural distinction between
values and computations, while we use type universes for that. Generally
speaking, type-based restrictions are easier to work with in dependent type
theories than structural restrictions.

\subsection{Staging}\label{sec:staging}

With what we have seen so far, there is no interaction between the meta and
object levels. We make such interaction possible with \emph{staging operations}.

\begin{itemize}
\item For $A : \Ty$, we have $\Lift A : \MTy$, pronounced as ``lift $A$''. This is
      the type of metaprograms that produce $A$-typed object programs.
\item For $A : \Ty$ and $t : A$, we have $\ql t \qr : \Lift A$, pronounced ``quote $t$''. This
      is the metaprogram which immediately returns $t$.
\item For $t : \Lift A$, we have $\spl t : A$, pronounced ``splice $t$''. This
  inserts the result of a metaprogram into an object term. \emph{Notation:}
  splicing binds stronger than function application, so $f\,\spl x$ is parsed as
  $f\,(\spl x)$.
\item We have $\ql \spl t \qr \equiv t$ and $\spl \ql t \qr \equiv t$ as definitional equalities.
\end{itemize}

A CFTT program is a mixture of object-level and meta-level top-level definitions
and declarations. \textbf{Staging} means running all metaprograms in splices and
inserting their output into object code, keeping all object-level top entries
and discarding all meta-level ones. Thus, staging takes a CFTT program as input,
and produces output which is purely in the object-level fragment, with no
metatypes and metaprograms remaining. The output is guaranteed to be
well-typed. This staging is formalized in detail in \cite{TODO}. In Section
\ref{TODO} we describe the modifications to ibid.\ that are used in this paper.

Let us look at some basic staging examples. Recall the meta-level identity
function; it can be used at the object-level too, by applying it to quoted
terms:
  \[ \letdef n : \Nat := \spl(\id\,\ql 10 + 10 \qr); ... \]
Here, $\id$ is used at type $\Lift \Nat$. During staging, the expression in the
splice is evaluated, so we get $\spl\ql 10 + 10 \qr$, which is definitionally
the same as $10 + 10$, which is our staging output here. Boolean
short-circuiting is another basic use-case:
\begin{alignat*}{3}
  &\mit{and} : \Up\Bool \to \Up\Bool \to \Up\Bool\\
  &\mit{and}\,x\,y = \ql\case \spl x\,\of \True \to \spl y; \False \to \False\qr
\end{alignat*}
Since the $y$ expression is inlined under a $\case$ branch at every use site, it
is only computed at runtime when $x$ evaluates to $\True$. In many situations,
staging can be used instead of laziness to implement short-circuiting, and with
generally better runtime performance, avoiding the overhead of thunking. Consider
the $\map$ function now:
\begin{alignat*}{3}
  & \hspace{-5em}\mathrlap{\map : \{A\,B : \VTy\} \to (\Up A \to \Up B) \to \Up (\List\,A) \to \Up(\List\,B)}\\
  & \hspace{-5em}\mathrlap{\map\,f\,\vas = \ql\letrec \go\,\vas := \case \vas\,\of}\\
  & \hspace{-5em}\ind\ind\ind\ind\ind \ind\Nil           &&\to \Nil;\\
  & \hspace{-5em}\ind\ind\ind\ind\ind \ind\Cons\,a\,\vas &&\to \Cons\,\spl(f\,\ql a \qr)\,(\go\,\vas);\\
  & \hspace{-5em}\ind\ind\ind\ind\ind\go\,\spl\vas \qr
\end{alignat*}
For example, this can be used as $\letdef f\,\vas : \List\,\Nat \to \List\,\Nat :=
\spl(\map\,(\lam x.\,\ql \spl x + 10 \qr)\,\qt{\vas}) $.  This is staged to a recursive
definition where the mapping function is inlined into the $\Cons$ case as
$\Cons\,a\,\vas \to \Cons\,(a + 10)\,(\go\,\vas)$.  Note that $\map$ has to
abstract over value types, since lists can only contain values, not functions. Also, the
mapping function has type $\Up A \to \Up B$, instead of $\Up (A \to B)$. The
former type is often preferable to the latter in staging; the former is
a metafunction with useful computational content, while the latter is merely a
black box that computes object code. If we have $f : \Up(A \to B)$, and $f$ is
staged to $\ql \lam x.\,t\qr$, then $\spl f u$ is staged to an
undesirable ``administrative'' $\beta$-redex $(\lam x.\,t)\,u$.

\section{Monads \& Monad Transformers}\label{sec:monad-transformers}

In this section we build a library for monads and monad transformers. We believe
that this is a good demonstration of CFTT-s abilities, since monads are
ubiquitous in practical Haskell programming, and they also introduce a great
amount of abstraction and that should be optimized away.

\subsection{Binding-Time Improvements}\label{sec:binding-time-improvements}

We start with some preparatory work before getting to monads. We saw that $\Up
A \to \Up B$ is usually preferable to $\Up(A \to B)$. The two types are actually
equivalent up to the runtime semantics of object programs, and we can convert
back and forth in CFTT:
\begin{alignat*}{3}
  &\mup : \Up (A \to B) \to \Up A \to \Up B && \ind\ind \mdown : (\Up A \to \Up B) \to \Up (A \to B) \\
  &\mup\,f\,a = \ql \spl f\, \spl a\qr   && \ind\ind \mdown\,f = \ql \lam a.\,\spl(f\,\ql a \qr) \qr
\end{alignat*}
We can not show internally, using propositional equality, that these functions are
inverses, since we do not have $\beta\eta$-rules for object functions; but we
will not need this proof in the rest of the paper.

In the staged compilation and partial evaluation literature, the term
\textbf{binding time improvement} is used to refer to such conversions, where
the ``improved'' version supports more compile-time computation. A general
strategy for generating efficient ``fused'' programs, is to try to work as much
as possible with improved representations, and only convert back to object code
at points where runtime control dependencies are unavoidable.

We use a Haskell-style type class for binding time improvements:
\begin{alignat*}{3}
  &\hspace{-8em}\mathrlap{\tyclass\,\Improve\,(A : \Ty)\,(B : \MTy)\,\where} \\
  &\hspace{-8em}\ind \mup   &&: \Up A \to B\\
  &\hspace{-8em}\ind \mdown &&: B \to \Up A
  &\\
  &\hspace{-8em}\mathrlap{\instance \Improve\,(A \to B)\,(\Up A \to \Up B)\,\where\,\,...}
\end{alignat*}
We will use type classes in an informal way, without precisely specifying how
they work. However, in the Haskell adaptation of this paper we do make crucial
use of type classes. There are some necessary differences between the CFTT and
Haskell versions though, which we will summarize in Section \ref{TODO}. Let us
look at improvement for product types now:
\begin{alignat*}{3}
  &\hspace{-4em}\mathrlap{\instance \Improve\,(A,\,B)\,(\Up A,\,\Up B)\,\where}\\
  &\hspace{-4em}\ind \mup\,x   && = (\ql \fst\,\spl x \qr,\, \ql \snd\,\spl x \qr)\\
  &\hspace{-4em}\ind \mdown\,(a,\,b) && = \ql(\spl a,\, \spl b)\qr
\end{alignat*}
Here we overload Haskell-style product type notation for types and pairing, at
both levels (products are definable as a value type at the object level). There
is a problem with this conversion though: $\mup$ uses $x : \Up(A,\,B)$ twice,
which can increase code size and duplicate runtime computations. For example,
$\mdown\,(\mup\,\ql f\,x \qr)$ is staged to $\ql (\fst\,(f\,x),\,\snd\,(f\,x))
\qr$. It would be safer to first $\mbf{let}$-bind an expression with type
$\Up(A,\,B)$, and then only use projections of the newly bound variable. This is
called \textbf{let-insertion} in staged compilation. But it is impossible to use
let-insertion in $\mup$ because the return type is a product in $\MTy$, and we
cannot introduce object binders in meta-level code.

\subsection{The Code Generation Monad}\label{sec:the-code-generation-monad}

Our solution is to use a monad which extends $\MTy$ with the ability to
freely generate object-level code and introduce object binders.
\begin{alignat*}{3}
  & \newtype \Gen\,A = \Gen\,\{\unGen : \{R : \Ty\} \to (A \to \Up R) \to \Up R\}
\end{alignat*}
This is a monad in $\MTy$ in a very standard sense, and we reuse Haskell type classes
and $\mdo$-notation.
\begin{alignat*}{3}
  &\instance \Monad\,\Gen\,\where\\
  &\ind \return a = \Gen\,(\lam k.\,k\,a)\\
  &\ind \mit{ga} \bind f = \Gen\,(\lam k.\,\unGen\,\mit{ga}\,(\lam a.\,(f\,a\,k)))
\end{alignat*}
From the $\Monad$ instance, the $\Functor$ and $\Applicative$ instances can be also derived.
We will reuse $(\fmap)$ and $(\ap)$ for applicative notation
as well.
An inhabitant of $\Gen\,A$ can be viewed as an action whose effect is to produce
some object code and returns an $A$ value that can depend on produced object
binders. $\Gen$ can be only ``run'' when the result type is an object type:
\begin{alignat*}{3}
  &\runGen : \Gen\,(\Up A) \to \Up A\\
  &\runGen\,\vma = \unGen\,\vma\,\id
\end{alignat*}
We can let-bind object expressions in $\Gen$:
\begin{alignat*}{3}
  & \gen : \Up A \to \Gen\,(\Up A) \\
  & \gen\,a = \Gen\,(\lam k.\,\ql \letdef x : A := \spl a; \spl(k\,\ql x \qr) \qr)
\end{alignat*}
And also recursive function definitions:
\begin{alignat*}{3}
  & \genRec : \{A : \CTy\} \to (\Up A \to \Up A) \to \Gen\,(\Up A) \\
  & \genRec\,f = \Gen\,(\lam k.\,\qt{\letrec x : A := \spl(f\,\qt{x}); \spl(k\,\qt{x})})
\end{alignat*}
Now, using do-notation, we may write $\mdo \{x \fro \gen\,\qt{10 + 10}; y \fro
\gen\,\qt{20 + 20}\};\return \qt{x + y}\}$, for a $\Gen\,(\Up \Nat)$
action. Running this with $\runGen$ yields $\qt{\letdef x := 10 + 10; \letdef y
  := 20 + 20; x + y}$. We can also define a ``safer'' binding-time improvement for
products, using let-insertion:
\begin{alignat*}{3}
  &\hspace{-4em}\mathrlap{\instance \Improve\,(A,\,B)\,(\Gen\,(\Up A,\,\Up B))\,\where}\\
  &\hspace{-4em}\ind \mup\,x   && = \mdo\{x \fro \gen\,x; \return (\qt{\fst\,\spl x},\,\qt{\snd\,\spl x})\}\\
  &\hspace{-4em}\ind \mdown\,x && = \runGen\,\$\,\mdo \{(a,\,b) \fro x; \return \qt{(\spl a,\, \spl b)}\}
\end{alignat*}

Working in $\Gen$ is convenient, since we can freely generate object code and
also have access to the full metalanguage. Also, the whole point of staging is
that \emph{eventually} all metaprograms will be used for the purpose of code
generation, so ultimately we always want to $\runGen$ our actions. So why not
just always work in $\Gen$? We may not want to do that, because we would like to
reason about the size and makeup of the code we are generating, and the
implicit code generation in $\Gen$ is not very transparent. This is a bit
similar to the $\msf{IO}$ monad in Haskell, where eventually everything needs
to run in $\msf{IO}$, but we may prefer to not write most of our program in
$\msf{IO}$.

\subsection{Monads}\label{sec:monads}

Let us start with the $\Maybe$ monad. We have $\data \Maybe\,A :=
\Nothing\,|\,\Just\,A$, and $\Maybe$ itself is available as a $\VTy \to \VTy$
metafunction. However, we cannot directly fashion a monad out of $\Maybe$, since
we do not have enough type formers in $\VTy$. We could try to use the following
type for binding:
\[ (\bind) : \Up(\Maybe\,A) \to (\Up\,A \to \Up\,(\Maybe\,B)) \to \Up(\Maybe\,B) \]
This works, but the definition necessarily uses runtime case splits on $\Maybe$
values, many of which could be optimized away during staging. Also, not having
a ``real'' monad is inconvenient for the purpose of code reuse.

Instead, our strategy is to only use proper monads in $\MTy$, and convert
between object types and meta-monads when necessary, as a form of binding-time
improvement. Assume that $\Maybe_M$ is a meta-level monad, and $\MaybeT_\M$ is
the monad transformer with the standard definition:
\[ \newtype \MaybeT_\M\,M\,A = \MaybeT_\M\,\{\runMaybeT_\M : M\,(\Maybe_\M\,A)\} \]
Then the object-level $\Maybe$ is binding-time improved as follows:
\begin{alignat*}{3}
  &\instance \Improve\,(\Maybe\,A)\,(\MaybeT_\M\,\Gen\,(\Up\,A))\,\where\\
  %% &\ind \mup : \Up\,(\Maybe\,A) \to \MaybeT_\M\,\Gen\,(\Up\,A) \\
  &\ind \mup\,x = \MaybeT_\M \dlr \Gen \dlr \lam k.\\
  &\ind\ind \qt{\case \spl x\,\of \Nothing \to \spl(k\,\Nothing_\M);\Just\,a \to \spl(k\,(\Just_\M\qt{a}))}\\
  %% &\ind \mdown : \MaybeT_\M\,\Gen\,(\Up\,A) \to \Up\,(\Maybe\,A) \\
  &\ind \mdown\,(\MaybeT_\M\,(\Gen\,\vma)) = \\
  &\ind\ind \vma\,(\lam x.\,\case x\,\of \Nothing_\M \to \qt{\Nothing}; \Just_\M\,a \to \qt{\Just\,\spl a})
\end{alignat*}
With this, we get the $\Monad$ instance for free from $\MaybeT_\M$ and $\Gen$. A small example:
\[ \letdef n : \Maybe\,\Nat := \spl(\mdown\,\$\,\mdo \{x \fro \return\,\qt{10}; y \fro \return\,\qt{20}; \return\,\qt{x + y})\});\,... \]
Since $\MaybeT_\M$ is meta-level, its binding always fully computes at staging time. Thus, the above code is staged to
\[ \letdef n : \Maybe\,\Nat := \Just\,(10 + 20);\;... \]
Assume also a $\lift : \Monad\,M \RA M\,A \to \MaybeT_\M\,M\,A$ operation, as in
Haskell. We shall assume this for all monad transformers henceforth. We can do
let-insertion in $\MaybeT_\M\,\Gen$ by simply lifting:
\begin{alignat*}{3}
  &\gen' : \Up A \to \MaybeT_\M\,\Gen\,(\Up A) \\
  &\gen'\,a = \lift\,(\gen\,a)
\end{alignat*}
However, it is more convenient to proceed in the style of Haskell's monad
transformer library \cite{TODO}, and have a class for monads that can do code
generation:
\begin{alignat*}{3}
&\tyclass\,\Monad\,M \RA \MonadGen\,M\,\where\\
&\ind \liftGen : \Gen\,A \to \M\,A\\
&\instance \MonadGen\,\Gen\,\where\,\liftGen = \id \\
&\instance \MonadGen\,M \RA \MonadGen\,(\MaybeT_\M\,M)\,\where\,\liftGen = \lift \circ \liftGen
\end{alignat*}
We also redefine $\gen$ and $\genRec$ to work in any $\MonadGen$, so from now on we have:
\begin{alignat*}{3}
 & \gen   &&: \MonadGen\,M \RA \Up A \to M\,(\Up A)\\
 &\genRec &&: \MonadGen\,M \RA (\Up A \to \Up A) \to M\,(\Up A)
\end{alignat*}

\subsubsection{Case splitting in monads}
Trying to write more substantial code, we quickly bump into an issue. We
want to case-split on object-level data inside a monadic action, like in the following:
\begin{alignat*}{3}
  & \hspace{-4em}\mathrlap{f : \Nat \to \Maybe\,\Nat}\\
  & \hspace{-4em}\mathrlap{f\,x := \spl(\mdown\,\$\, \case x == 10\,\of}\\
  & \hspace{-4em}\ind \True  &&\to \return\,\qt{x + 5};\\
  & \hspace{-4em}\ind \False &&\to \Nothing_\M)
\end{alignat*}
This is ill-typed as written, since we cannot compute meta-level actions from an
object-level case split. Fortunately, with a little bit more work, case
splitting on object values is actually possible in any $\MonadGen$, on any value
type.

We demonstrate this for lists first. An object-level $\mbf{case}$ on lists introduces
two points where code generation can continue. We define a metatype which gives
us a ``view'' on these points:
\[ \data \msf{SplitList}\,A = \Nil'\,|\,\Cons'\,(\Up A)\,(\Up (\List\,A)) \]
We can generate code for a case split, returning a view on it:
\begin{alignat*}{3}
  &\mit{split} : \Up (\List\,A) \to \Gen\,(\msf{SplitList}\,A)\\
  &\mit{split}\,\vas = \Gen \dlr \lam k.\,\qt{\case \spl \vas\,\of \Nil \to \spl(k\,\Nil'); \Cons\,a\,\vas \to \spl(k\,(\Cons'\,\qt{a}\,\qt{\vas}))}
\end{alignat*}
Now, in any $\MonadGen$, we may write
\begin{alignat*}{3}
  &\mdo \{\mit{sp} \fro \liftGen\;(\!\mit{split}\,\vas);(\case \mit{sp}\,\of \Nil' \to ...;\Cons'\,a\,\vas \to ...)\}
\end{alignat*}
%% \begin{alignat*}{3}
%%   &\mdo && \mit{sp} \fro \liftGen \dlr \mit{split}\,\vas\\
%%   &     && \case \mit{sp}\,\of \\
%%   &     && \ind \Nil' \to ... \\
%%   &     && \ind \Cons'\,a\,\vas \to ...
%% \end{alignat*}
This can be generalized to any splitting on object values. In the supplemental
Haskell implementation, we overload $\mit{split}$ with a type class. However, in
a native implementation of CFTT it might make sense to simply extend
$\mdo$-notation with an elaboration of $\mbf{case}$ on $\Up A$-typed data to an
application of the appropriate $\mit{split}$ function, so we can simply write a
$\case\,\vas\,\of\,...$ in a $\mdo$-block. We adopt this in the rest of this
paper.

\subsection{Monad Transformers}\label{monad-transformers}

At this point, it makes sense to aim for a monad transformer library where
binding-time improvement is defined compositionally, by recursion on the
transformer stack. The base case is the following:
\begin{alignat*}{3}
  & \newtype \Identity\,A := \Identity\,\{\runIdentity : A\} \\
  & \instance \Improve\,(\Identity\,A)\,(\Gen\,(\Up A))\,\where\\
  & \ind \mup\,x = \Gen \dlr \lam k.\,k\,\qt{\runIdentity\,\spl x}\\
  & \ind \mdown\,x = \unGen\,x \dlr \lam a.\,\qt{\Identity\,\spl a}
\end{alignat*}
We also switch from the object-level $\Maybe$ to a ``transformer'' form as well,
and recover it as $\MaybeT\,\Identity$.
\[ \newtype \MaybeT\,M\,A := \MaybeT\,\{\runMaybeT : M\,(\Maybe\,A)\} \]
Generally, we want to improve an operator $F : \VTy \to \VTy$ to some $M : \MTy
\to \MTy$, where $M$ is a $\MonadGen$. We define a \emph{quantified class
constraint} \cite{TODO} for this, that we will often use as an assumption:
\[ \ImproveF\,F\,M = \MonadGen\,M \times ((A : \VTy) \to \Improve\,(F\,A)\,(M\,(\Lift\,A))) \]
With this, improvement can be generally defined for $\MaybeT$:
\begin{alignat*}{3}
  &\instance \ImproveF\,F\,M \RA \Improve\,(\MaybeT\,F\,A)\,(\MaybeT_\M\,M\,(\Up A))\,\where\\
  &\ind \mup\,x = \MaybeT_\M \dlr \mdo\\
  &\ind\ind \vma \fro \mup\,\qt{\runMaybeT\,\spl x}\\
  &\ind\ind \case \vma\,\of \Nothing \to \return \Nothing_\M\\
  &\hspace{6.5em}         \Just\,a \hspace{1.15em}\to \return (\Just_\M\,a)\\
  &\ind \mdown\,(\MaybeT_\M\,x) = \qt{\MaybeT\,\spl(\mdown \dlr x \bind \lam x.\,\case x\,\of\\
  &\ind\ind\Nothing_\M \to \return \qt{\Nothing}\\
  &\ind\ind\Just_\M\,a\hspace{1.15em} \to \return \qt{\Just\,\spl a})}
\end{alignat*}
In the $\mbf{case}$ in $\mup$, we already use our syntax sugar for matching on a $\Maybe$
inside an $\M$ action. This is legal, since we know from the $\ImproveF\,F\,M$ assumption
that $M$ is a $\MonadGen$.

In the meta level, we can reuse essentially all definitions from Haskell's
monad transformer library $\msf{mtl}$. The additional work that we have to do in
the CFTT setting:
\begin{itemize}
  \item Adding $\MonadGen$ instances, but these are trivially defined as
        $\liftGen = \lift \circ \liftGen$ everywhere.
  \item Defining the object-level counterparts of monad transformers, and
        also the $\Improve$ instances.
\end{itemize}
From $\msf{mtl}$, only the continuation monad transformer fails to support
binding-time-improvement in CFTT, because of the obvious need for dynamic
closures. In the following we present only $\StateT$ and $\ReaderT$. Starting
with $\StateT$, we assume $\StateT_\M$ as the standard meta-level definition.
The object-level $\StateT$ has type $(S : \VTy)(F : \VTy \to \VTy)(A : \VTy) \to
\VTy$; the state parameter $S$ has to be a value type, since it is an input to
an object-level function.
\begin{alignat*}{3}
  &\instance \ImproveF\,F\,M \RA \Improve\,(\StateT\,S\,F\,A)\,(\StateT_\M\,(\Up S)\,M\,(\Up A))\,\where\\
  &\ind \mup\,x = \StateT_\M \dlr \lam s.\,\mdo\\
  &\ind\ind \mit{as} \fro \mup\,\qt{\runStateT\,\spl x\,\spl s}\\
  &\ind\ind \case \mit{as}\,\,\of (a,\,s) \to \return (a,\,s)\\
  &\ind \mdown\,x = \qt{ \StateT\,(\lam s.\, \spl(\mdown \dlr \mdo\\\
  &\ind \ind (a,\,s) \fro \runStateT_\M\,x\,\qt{s}\\
  &\ind \ind \return \qt{(\spl a,\, \spl s)}))}
\end{alignat*}
Like before in $\MaybeT$, we rely on object-level case splitting in the
definition of $\mup$. For $\Reader$, the environment parameter also has to be a
value type, and we define improvement as follows.
\begin{alignat*}{3}
  &\instance \ImproveF\,F\,M \RA \Improve\,(\ReaderT\,R\,F\,A)\,(\ReaderT_\M\,(\Up R)\,M\,(\Up A))\,\where\\
  &\ind \mup\,x   = \ReaderT_\M \dlr \lam r.\, \mup\,\qt{\runReaderT\,\spl x\,\spl r}\\
  &\ind \mdown\,x = \qt{\ReaderT\,(\lam r.\,\spl(\mdown\,(\runReaderT_M\,x\,\qt{r})))}
\end{alignat*}

\subsubsection{$\State$ and $\Reader$ operations} Now, if we try to use
the $\modify$ function that we already have for $\State_\M$, a curious thing
happens. The meaning of $\modify\,(\lam x.\,\qt{\spl x + \spl x})$ is to replace
the current state $x$, as an object expression, with the expression $\qt{\spl x
  + \spl x}$, and this happens at staging time. This behaves as an ``inline''
modification which replaces every subsequent mention of the state with a different
expression. For instance, ignoring newtype wrappers for now,
\[ \mdown \dlr \mdo \{\modify\,(\lam x.\,\qt{\spl x + \spl x}); \modify\,(\lam x.\,\qt{\spl x + \spl x});\return\,\qt{()}\} \]
is staged to
\[ \qt{\lam x.\,((),\,(x + x) + (x + x))} \]
which duplicates the evaluation of $x + x$. The solution is to force the
evaluation of the new state in the object language, by let-insertion. A similar
phenomenon happens with the $\mit{local}$ function in $\Reader$. So we define
``stricter'' versions of these operations. We also return $\Up ()$ from
actions instead of $()$ --- the former is more convenient, because it can
be immediately lowered with $\mdown$.
\begingroup
\allowdisplaybreaks
\begin{alignat*}{3}
  & \mput' : (\MonadState\,(\Up S)\,M,\,\MonadGen\,M) \RA \Up S \to M\,(\Up ()) \\
  & \mput'\,s = \mdo \{s \fro \gen\,s; \mput\,s; \return \qt{()}\}\\
  &\\
  & \modify' : (\MonadState\,(\Up S)\,M,\,\MonadGen\,M) \RA (\Up S \to \Up S) \to M\,(\Up ()) \\
  & \modify'\,f = \mdo \{s \fro \get; \mput'\,(f\,s)\}\\
  &\\
  & \mit{local'} : (\MonadReader\,(\Up R)\,M,\,\MonadGen\,M) \RA (\Up R \to \Up R) \to M\,A \to M\,A\\
  & \mit{local'}\,f\,\vma = \mdo \{r \fro \mit{ask}; r \fro \gen\,(f\,r);\mit{local}\,(\lam \_.\,r)\,\vma\}
\end{alignat*}
\endgroup
Now,
\[ \mdown \dlr \mdo \{\modify'\,(\lam x.\,\qt{\spl x + \spl x}); \modify'\,(\lam x.\,\qt{\spl x + \spl x})\} \]
is staged to
\[ \qt{\lam x.\,\letdef x := x + x; \letdef x := x + x; ((),\,x)} \]

\subsection{Joining Control Flow in Monads}\label{sec:joining-control-flow-in-monads}

There is a deficiency in our library so far. Assuming $b : \Bool$, consider:
\begin{alignat*}{3}
  & \mdown \dlr \mdo \\
  & \ind \case b\,\of\\
  & \ind \ind \True  \hspace{0.2em}\to \mput' \qt{10}\\
  & \ind \ind \False \to \mput' \qt{20}\\
  & \ind \modify' (\lam x. \qt{\spl x + 10})
\end{alignat*}
The $\modify' (\lam x. \qt{\spl x + 10})$ action gets inlined in both case
branches during staging! This follows from the definition of monadic binding in
$\Gen$ and the $\mit{split}$ function in the desugaring of $\mbf{case}$. Code
generation is continued in two branches with the same action. If we have
multiple $\mbf{case}$ splits sequenced after each other, that yields exponential
code size. Right now, we can fix this by let-binding the problematic action:
\begin{alignat*}{3}
  & \mdown \dlr \mdo \\
  & \ind x \fro \gen \dlr \mdown \dlr \case b\,\of\\
  & \ind \ind \True  \hspace{0.2em}\to \mput' \qt{10}\\
  & \ind \ind \False \to \mput' \qt{20}\\
  & \ind \mup\,x\\
  & \ind \modify' (\lam x. \qt{\spl x + 10})
\end{alignat*}
This removes code duplication by round-tripping through an object-level
$\mbf{let}$. However, while this solution is fairly good in a state monad, where
we can reasonably expect that product types can be unboxed by the object
compiler, it introduces unnecessary runtime constructors for $\Maybe$ and
other monads containing sum types. For $\Maybe$, $\mdown$ introduces a runtime
$\Just$ or $\Nothing$, and $\mup$ introduces a runtime case split. A better
solution would be to introduce two let-bound \emph{join points} before the
offending $\mbf{case}$, one for returning a $\Just$ and one for returning
$\Nothing$, but fusing away the actual runtime constructors. So, we would like
to produce object code for a $\mbf{case}$ which looks like the following:
\begin{alignat*}{3}
  & \letdef\,\mit{joinJust}\,n := ...\\
  & \letdef\,\mit{joinNothing}\,() := ...\\
  & \case x == 10\,\of\\
  & \ind \True \hspace{0.2em}\to \mit{joinJust}\,(x + 10) \\
  & \ind \False \to \mit{joinNothing}\,()
\end{alignat*}
Such fused returns are possible whenever we have a $\Gen\,A$ action at the
bottom of the transformer stack, such that $A$ is isomorphic to a finite
meta-level sum of value types. Recall that $\Gen\,A$ is defined as $\{R : \VTy\}
\to (A \to \Up R) \to \Up R$. Here, the $A \to \Up R$ continuation represents
all possible code points where we can return to, and if $A$ is a finite sum, we
can rearrange $A \to \Up R$ to a finite product of functions. Hence, we need a modest
amount of generic programming with sums of values and products of computations.
\begin{alignat*}{3}
  & \data \SumVS : \List\,\VTy \to \MTy\,\where\,...\\
  & \data \ProdCS : \List\,\CTy \to \MTy\,\where\,...
\end{alignat*}
%% \begin{alignat*}{3}
%%   & \mathrlap{\data \SumVS : \List\,\VTy \to \MTy\,\where}\\
%%   & \ind \Here &&: \Lift A \to \SumVS\,(\Cons\,A\,\mAs) \\
%%   & \ind \There &&: \SumVS\,\mAs \to \SumVS\,(\Cons\,A\,\mAs)\\
%%   & \mathrlap{\data \ProdCS : \List\,\CTy \to \MTy\,\where}\\
%%   & \ind \Nil  &&: \ProdCS\,\Nil \\
%%   & \ind \Cons &&: \Lift A \to \ProdCS\,\mAs \to \SumVS\,(\Cons\,A\,\mAs)
%% \end{alignat*}
%% Here, we overload $\List$ to refer to meta-level lists, dropping the $_\M$
%% subscripts. We also overload the list constructors for $\ProdCS$.
We use a class for types that have a $\SumVS$ representation:
\begin{alignat*}{3}
  & \hspace{-3em}\mathrlap{\tyclass\,\IsSumVS\,(A : \MTy)\,\where} \\
  & \hspace{-3em}\ind \Rep    &&: \List\,\VTy \\
  & \hspace{-3em}\ind \encode &&: A \to \SumVS\,\Rep \\
  & \hspace{-3em}\ind \decode &&: \SumVS\,\Rep \to A
\end{alignat*}
We also define the mentioned isomorphism for $\Gen$ continuations.
\begin{alignat*}{3}
  &\mtabulate &&: (\SumVS\,\mAs \to \Lift B) \to \ProdCS\,(\map\,(\lam\,A.\,A\to B)\,\mAs)\\
  &\mindex    &&: \ProdCS\,(\mit{map}\,(\lam\,A.\,A\to B)\,\mAs) \to (\SumVS\,\mAs \to \Lift B)
\end{alignat*}
We omit the definitions of $\mtabulate$, $\mindex$ and the $\IsSumVS$ instances,
since they amount to standard generic programming; see e.g. \cite{TODO}. The
last basic definition we need is to let-bind all computations in a generic
product:
\begin{alignat*}{3}
  & \mathrlap{\mit{genProdCS} : \ProdCS\,\mAs \to \Gen\,(\ProdCS\,\mAs)}\\
  & \mit{genProdCS}\,\Nil                 &&= \return\,\Nil\\
  & \mit{genProdCS}\,(\Cons\,c\,\mit{cs}) &&= \Cons \fmap \gen\,c \ap \mit{genProdCS}\,\mit{cs}
\end{alignat*}
We introduce a class for monads that support control flow joining.
\[ \tyclass\,\MonadJoin\,M\,\where\,\join : \IsSumVS\,A \RA M\,A \to M\,A \]
The most interesting instance is for $\Gen$.
\begin{alignat*}{3}
  &\instance \MonadJoin\,\Gen\,\where\\
  &\ind \join\,\vma = \Gen \dlr \lam k.\,\runGen \dlr \mdo\\
  &\ind \ind \mit{joinpoints} \fro \mit{genProdCS}\,(\mtabulate\,(k \circ \decode))\\
  &\ind \ind a \fro \vma\\
  &\ind \ind \return \dlr \mindex\,\mit{joinpoints}\,(\encode\,a)
\end{alignat*}
Here we first convert $k : A \to \Lift R$ to a product of join points and
let-bind each of them. Then we adjust the return code points of $\vma$ by
looking up the corresponding continuation from the list of join points, using
$\mindex$. We also need $\MonadJoin$ instances for the other transformers in our
library, but these are all essentially just lifting $\join$. In $\StateT_\M$ we
must have $\IsSumVS$ for $S$ because $S$ appears in the transformer stack's
return value.
\begin{alignat*}{3}
  & \instance\,(\MonadJoin\,M) \RA \MonadJoin\,(\MaybeT_\M\,M)\,\where\\
  & \ind \join\,(\MaybeT_\M\,\vma) = \MaybeT_\M\,(\join\,\vma)\\
  & \instance\,(\MonadJoin\,M) \RA \MonadJoin\,(\ReaderT_\M\,R\,M)\,\where\\
  & \ind \join\,(\ReaderT_\M\,\vma) = \ReaderT_\M\,(\join \circ \vma)\\
  & \instance\,(\MonadJoin\,M,\,\IsSumVS\,S) \RA \MonadJoin\,(\StateT_\M\,S\,M)\,\where\\
  & \ind \join\,(\StateT_\M\,\vma) = \StateT_\M\,(\join \circ \vma)
\end{alignat*}
Now, whenever the return value of a $\mbf{case}$-splitting action is a sum of
values (this includes just returning an object value, which is by far the most
common situation), we can use $\join \dlr \case x\,\of ...$ to block inlining
into the branches, without creating any runtime sum types.

\todo{bigger example}

\subsection{Discussion}

So far, we have a monad transformer library with the following features:
\begin{itemize}
\item Almost all definitions from the well-known Haskell ecosystem of monads and monad transformers
      can be directly reused, in the meta level.
\item We can quite liberally pattern match on object-level values in monadic code, insert object-level $\mbf{let}$-s
      with $\gen$ and control code size by $\join$-s.
\item In monadic code, object-level data constructors are only ever created by $\mdown$, and switching on object-level data is only
      created by $\mit{split}$ and $\mup$. Everything else is fully fused, and all function calls can be compiled to
      statically known saturated calls.
\end{itemize}

As to potential weaknesses, first, the system as described in this section has
quite a bit of syntactic noise and requires extra attention from programmers.
We believe that the code noise can be mitigated very effectively in a native
CFTT implementation. Kovács \cite{TODO} demonstrated in a prototype elaborator
that almost all quotes and splices are unambiguously inferable in 2LTT programs,
if we make the concession that stages of let-definitions are always specified
(as we do here). Moreover, $\mup$ and $\mdown$ should be also effectively
inferable, using bidirectional elaboration, but it would make sense to limit
inferred binding-time improvement to monads only. With stage inference and
binding-time improvement inference, code in CFTT would look only modestly more
complicated than in Haskell, and we would get far more robust and controllable
code generation.

Second, in CFTT we cannot store most monadic actions in runtime data structures,
those that are implemented using functions, nor can we have functions in
$\State$ state, or in $\Reader$ environments. However:
\begin{itemize}
  \item Essential usage of this is rare in practice.
  \item It is possible to extend CFTT with a closure type former, converting computations to values,
        in which case there is no such limitation anymore. Here, closure-freedom is still available;
        we can choose to use or abstain from the explicit closure type.
\end{itemize}

\subsection{Haskell Implementation}

We implemented all features in this section in typed Template Haskell
\cite{TODO}, using only the strongly typed interface. The organization of the
code is essentially the same as here, but some changes are necessary:
\begin{itemize}
  \item TH cannot properly handle scoping of type variables across quotations,
        generating ill-typed code in some cases. It is fairly easy to work around
        this using term-level proxy values.
  \item We do not bother tracking value and computation types. This could be done,
        using dummy type classes, but we would not get any extra guarantees from it
        in downstream compilation.
  \item When we take the product of generic sums in CFTT, we simply take the
        object-level product of component types, pairwise. We do this because we
        assume that the downstream compiler can represent products without
        overheads, in an unboxed manner, and also erase unit types. In Haskell we do
        not rely on this, and instead use a slightly more complicated
        sum-of-products-of-values representation, and also fuse away product types
        during staging.
  \item We make heavy use of \emph{singleton types} \cite{TODO} in generic sums-of-products, to emulate
        dependent types. This adds significant noise compared to a native dependently typed
        version.
  \item We have two separate type classes for binding-time improvement, one for
        base types and one for monads specifically. This is because the quantified
        constraints in \ref{TODO} yield poor type inference for $\mup$ and $\mdown$,
        by preventing \emph{functional dependencies} \cite{TODO} between the class
        parameters. We expect that in a native CFTT implementation, the extra staging
        information in type universes would help inference in this case.
\end{itemize}

\section{Stream Fusion}

Stream fusion refers to a collection of techniques for generating efficient code
from declarative definitions involving streams of values, where intermediate
data structures are eliminated. For example, mapping twice over a list produces
an intermediate list in a naive implementation but does not do so in a fused
implementation.

Fusion techniques can be broadly grouped into \emph{push} fusion, which is based
on Church-encodings of inductive lists, and $\emph{pull}$ fusion, which is based
on Church-encodings of coinductive lists, which manifests as \emph{state
machines}. The two styles have different trade-offs, and in practical
programming it is a good idea to support both, but in this section we focus on
pull fusion.

The reason is partly lack of space, but also that pull fusion has been
historically the more difficult to reliably compile, and we can demonstrate
significant improvements in CFTT, in robustness and expressive power. Our fusion
always goes through, and we also support a $\mit{concatMap}$ operation, which
has been elusive in guaranteed fusion. We believe that our implementation also
provides valuable conceptual clarity.

\subsection{Streams}

A pull stream is a meta-level specification of a state machine:
\begin{alignat*}{3}
  & \data \Step\,S\,A = \Stop\,|\,\Yield\,A\,S\,|\,\Skip\,S\\
  & \data \Pull\,(A : \MTy) : \MTy\,\where\\
  & \ind \MkPull : (S : \MTy) \to \IsSumVS\,S \RA S \to (S \to \Gen\,(\Step\,S\,A)) \to \Pull\,A
\end{alignat*}
In $\MkPull$, $S$ is the type of the internal state which is required to be a
finite sum of values by the $\IsSumVS\,S$ constraint. The next $S$-typed field
is the initial state, while $S \to \Gen\,(\Step\,S\,A)$ represents all
transitions. Possible transitions are stopping ($\Stop$), transitioning to a
new state while outputting a value ($\Yield$) and making a silent transition to
a new state ($\Skip$).


\section{TODOS}

\todo{Introduce newtypes accordingly}
\todo{allow arbitrary parameters (including ValTy -> ValTy functions) for object ADTs}
\todo{extend as I have more stuff}
\todo{FIGURE OUT: monadic tailrec}
\todo{FIGURE OUT: Traversals}
\todo{DEVELOP: mutual letrec based on computational product types}


%% \section{The Object Language}\label{sec:the-object-language}

%% Describe object language. Simple types + first-order fun + fixpoints + ADT-s.

%% Show Agda formalization basically, types and syntax.

%% Show Agda definitional interpreter. Discuss closure-free evaluation

%% Examples, properties. Maybe big-step reduction? Maybe program equivalence?  Look
%% at Harper PFPL for program equivalence reference.

%% Sketch downstream compilation.
%% \begin{itemize}
%% \item Eta-expand function definitions to arity.
%% \item Turn functions in immediate beta-redexes to let-bindings.
%% \item Lambda-lift non-tail-called functions to top.
%% \item Keep only-tail-called local functions as join points.
%% \end{itemize}

%% Downstream compilation should erase the unit type and implement products as
%% ``flat'' unboxed structures.

%% Discuss object language design choices.
%% \begin{itemize}
%% \item Can be more obviously first-order: e.g.\ no currying, only Val->Val
%%   functions, or ANF functions. This requires more bureaucracy in
%%   metaprogramming.
%% \item Can be more flexible but less obviously first-order: allow function let
%%   bodies, allow function case bodies. Requires more translation. More convenient
%%   to program in. In future work.
%% \end{itemize}

%% \section{The Staged Language}\label{sec:the-staged-language}

%% Don't give horizontal-bar rules.

%% Meta-features:
%% \begin{itemize}
%% \item Pi, Sigma, Unit, Bottom, Universes (indices elided), ADT-s. Ind families probably not needed.
%% \item Agda notation.
%% \item Type classes informally in Haskell notation?
%% \end{itemize}

%% Explain staging ops, staging algorithm. Explain object embedding. Refer to 2LTT
%% paper for correctness.

%% Mention stage annotation inference, but we'll not use it often! To make things
%% explicit.

%% We should include a generous amount of staging examples, and some traced staging.

%% Basic staging examples: identity, map, power function. Boolean short-circuiting.

%% Basic staged classes, Eq, Ord, Show, Monoid, etc. Vectors with computed length.

%% \section{Programming With Monads}\label{sec:programming-with-monads}

%% \subsection{Basic Binding Time Improvements}\label{sec:basic-binding-time-improvements}

%% Notion.
%% For functions.
%% For products.
%% Note that for products there is a computation duplication problem.
%% Mention let-insertion.

%% \subsection{The Code Generation Monad}\label{sec:the-code-generation-monad}

%% Is fundamental to this work.

%% Definition. Explanation. Define Functor, Applicative, Monad instances.
%% Define runGen, runGen1, runGenN etc. Define let-insertion.

%% Basic examples.

%% Explanation in terms of Church-coding.

%% Gen Vector mapping or folding example from 2LTT repo. Without Gen, it's a
%% big pain to get ideal linear-size map code in N. With Gen it's easy-peasy.

%% \subsection{Monad Transformers}\label{sec:monad-transformers}

%% Identity, ReaderT, StateT, MaybeT, ExceptT

%% Strict versions of put, modify, local, runner functions.

%% MTL-style overloading. MonadReader, MonadState, MonadError, MonadGen, liftGen

%% Binding time improvement for monads. Up/down overloading.

%% \subsection{Case Splitting in MonadGen}\label{sec:case-splitting-in-monadgen}

%% It works for any object ADT.

%% Introduce sums-of-values (SOV).

%% Introduce syntax sugar for object case splits in MonadGen blocks.

%% Basic examples.

%% \subsection{Joining Control Flow}\label{sec:rejoining-control-flow}

%% The problem of join points. Motivation in terms of MaybeT Gen.

%% We can let-bind computations by tripping through up/down of monads,
%% but that introduces unbox/rebox cost for Maybe \& Either.

%% Works OK for Reader/State though.

%% \begin{verbatim}
%% f : Bool -> MaybeT0 Gen ^Int
%% f = \b. ~(down do
%%   x <- case <b> of
%%     True  -> gen <10>
%%     False -> gen <20>
%%   y <- gen <~x + 10>
%%   z <- gen <~x * 10>
%%   pure <~x + ~y + ~z>)
%% \end{verbatim}

%% Join points using SOV-s. Description of SOV machinery. MonadJoin.
%% instance MonadJoin Gen. MonadJoin MaybeT.

%% \subsection{Monadic Tail Recursion}\label{sec:monadic-tail-recursion}

%% \texttt{replicateM\_}, \texttt{find} functions working in ``any'' monad.
%% Non-tail recursion is not possible to fuse, return addresses are dynamic.
%% Tail recursion can return to statically known points.

%% \subsection{Discussion}

%% Limitations. We can't put non-value actions into structures, can't accumulate them.

%% Guarantees/properties. Closure-freedom. No administrative redexes. No boxing in join points.
%% Only boxing in monadic code comes from ``down''. No code blowup.

%% \section{Fusion}\label{sec:push-pull-fusion}

%% Motivation.


%% \subsection{Push streams}\label{sec:push-streams}

%% Definition.

%% \subsection{Pull streams}\label{sec:pull-streams}

%% Pull streams are definitely not monadic. \texttt{Nat -> Stream a} contains
%% ``infinite'' amount of code internally.

%% Pull streams are also not selective functors.

%% We do have monadic binding for lifted types, plus ``case'' matching
%% on arbitrary object values! This is practically almost as good as
%% having unlimited monadic bind! We need to assume a ``non-dependency axiom'',
%% that any $f : \Lift A \to \msf{ValTy}$ must be constant.

%% \todo{Validate non-dependency in the psh model!}
%% \todo{Add meta-identity type to the psh model}

\interlinepenalty=10000
\bibliography{references}

\end{document}
\endinput
