
\documentclass{easychair}

\usepackage{doc}
\usepackage{scalerel}
\usepackage{amsfonts}

\newcommand{\easychair}{\textsf{easychair}}
\newcommand{\miktex}{MiK{\TeX}}
\newcommand{\texniccenter}{{\TeX}nicCenter}
\newcommand{\makefile}{\texttt{Makefile}}
\newcommand{\latexeditor}{LEd}
\newcommand{\emptycon}{\scaleobj{.75}\bullet}
\newcommand{\ext}{\triangleright}
\newcommand{\arri}{\Rightarrow}

\newcommand{\ToS}{\mathsf{ToS}}
\newcommand{\U}{\mathsf{U}}
\newcommand{\Code}{\mathsf{Code}}
\newcommand{\Ty}{\mathsf{Ty}}
\newcommand{\V}{\mathsf{V}}
\newcommand{\C}{\mathsf{C}}

\title{Using Two-Level Type Theory for Staged Compilation %
  \thanks{The author was supported by the European Union, co-financed
    by the European Social Fund (EFOP-3.6.3-VEKOP-16-2017-00002).}}

\author{
Andr\'as Kov\'acs
}

% Institutes for affiliations are also joined by \and,
\institute{
  E\"otv\"os Lor\'and University,
  Budapest, Hungary \\
  \email{kovacsandras@inf.elte.hu}
}

\authorrunning{Kov\'acs}
\titlerunning{Using Two-Level Type Theory for Staged Compilation}
\pagenumbering{gobble}
\begin{document}

\maketitle

Two-level type theory \cite{twolevel} (2LTT) is a system for performing
metatheoretic constructions and reasoning involving certain object-level type
theories. Such reasoning is always possible by simply embedding object theories
in metatheories, but in that case we have to explicitly handle a deluge of
technical details about the object theory, most notably substitutions. If
everything that we aim to do is natural with respect to object-level
substitution, we can instead use 2LTT, which can be viewed as a notation for
working with presheaves over the object-level category of substitutions.

Likewise in metaprogramming, there is a spectrum: we can simply write programs
which output raw source code, or use staging instead, which is safer and more
convenient, but also restricted in some ways. In the current work we observe
that 2LTT is a powerful model for generative staged compilation.

\emph{Basic rules of 2LTT}. We have universes $\U^s_{i}$, where $s \in \{0,1\}$,
denoting a \emph{stage} or level in the 2LTT sense, and $i \in \mathbb{N}$
denotes a usual level index of \emph{sizing} hierarchies. The two dimensions of
indexing are orthogonal, and we will elide the $i$ indices in the following. We
assume Russell-style universes. For each $\Gamma \vdash A : \U^0$, we have
$\Gamma \vdash \Code\,A : \U^1$. Quoting: for each $\Gamma \vdash t : A$, we
have $\Gamma \vdash\,<\!t\!> : \Code\,A$. Unquoting: for each $\Gamma \vdash t :
\Code\,A$, we have $\Gamma \vdash\,\sim\!t : A$. Moreover, quoting and unquoting
form an isomorphism up to definitional equality. $\U^0$ and $\U^1$ can be closed
under arbitrary additional type formers.

The idea of staging is the following: given a closed $A : \U^0$ with a closed $t
: A$ in 2LTT, there are unique $A'$ and $t'$ in the object theory, which become
definitionally equal to $A$ and $t$ respectively after being embedded in
2LTT. In short, every meta-level construction can be computed away. Annekov et
al. \cite{twolevel} showed this without uniqueness of $A'$ and $t'$. We can get
uniqueness as well, using the normalization of 2LTT: by induction on the
(unique) normal forms of $A$ and $t$, we can show that they cannot contain
meta-level subterms. This shows that normalization is a sound staging algorithm,
but in practice we do not want to compute full normal forms; we want to compute
\emph{meta-level redexes only}. This can be done with a variation of standard
normalization-by-evaluation \cite{abel2013normalization,
  Wieczorek:2018:CFN:3176245.3167091} which also keeps track of stages.

\subsubsection*{Applications}

\textbf{Control over inlining and compile-time computation.} We can define two
variations of the polymorphic identity function for object-level types:
\begin{alignat*}{3}
  & \mathit{id} : (A : \U^0) \to A \to A \hspace{2em}&& \mathit{id'} : (A : \Code\,\U^0) \to \Code\,\sim\!A \to \Code\,\sim\!A\\
  & \mathit{id} = \lambda\,A\,x.\,x      && \mathit{id'} = \lambda A\,x.x
\end{alignat*}
The second version is evaluated at compile time. For example,
$\sim\!(\mathit{id'}<\!\mathsf{Bool^0}\!>\,<\!\mathsf{true^0}\!>)$ can be used
in object-level code, which is computed to $\mathsf{true^0}$ by staging. We can
also freely use induction on meta-level values to generate object-level code,
including types. Hence, 2LTT supports full dependent types (with universes and
large elimination) in staging.

\textbf{Monomorphization.} We assume now that the object language is a
\emph{simple type theory}. In this case, there is no universe $\U^0$ in the
object-level, so there is no $\Code\,\U^0$, but we can still freely include a
meta-level type $\Ty^0$ whose terms are identified with object-level types. Now,
meta-level functions can be used for quantification over object-level types, as in
$\mathit{id} : (A : \Ty^0) \to \Code\,\sim\!A \to \Code\,\sim\!A$. However,
since the object theory is simply typed and monomorphic, all polymorphism is
guaranteed to compute away during staging.

\textbf{Control over lambda lifting and closure creation}. We assume now a
dependent type theory on both levels, but with a \emph{first-order function
type} on the object level. This is defined by splitting $\U^0$ to a universe
$\V^0$ which is closed under inductive types but not functions, and a universe
$\C^0$ which has $V^0$ as a sub-universe, and is closed under functions with
domains in $\V^0$ and codomains in $\C^0$. The object theory supports
compilation which requires only lambda lifting, but no closures. On its own, the
object theory is fairly restricted, but together with staging we have a
remarkably expressive system. Then, we can close $\V^0$ under a separate type
former of closure-based functions, thereby formally distinguishing
lambda-liftable functions from closure-based functions. This enables typed
analysis of various optimization and fusion techniques. E.g.\ we get guaranteed
closure-freedom in code output if a certain fusion technique can be formalized
with only first-order function types. In particular, this may obviate the need
for arity analysis \cite{DBLP:journals/cl/Breitner18} in fold-based fusion.

\textbf{Memory layout control}. We assume again a dependent theory on both
levels, but now index $\U^0$ over \emph{memory layouts}. For example,
$\U^0\,\mathsf{erased}$ may contain runtime-erased types, and
$\U^0\,(\mathsf{word64} \times \mathsf{word64})$ may contain types represented
as unboxed pairs of machine words. We assume a meta-level type of
layouts. Hence, we can abstract over layouts, but after staging every layout
will be concrete and canonical in the output. This can be viewed as a more
powerful version of \emph{levity polymorphism} in GHC \cite{levity}, and a way
to retain both dependent types and non-uniform memory layouts in the object
theory.

\subsubsection*{Potential extensions}

\textbf{More stages, stage polymorphism}. The standard presheaf semantics of
2LTT can be extended to more levels in a straightforward way. It seems feasible
to also allow quantifying over all smaller levels, at a given level.

\textbf{Stage inference}. $\Code$ preserves all negative type formers up to
definitional isomorphism \cite{twolevel}, e.g. $\Code\,(A \to B) \simeq
(\Code\,A \to \Code\,B)$. This can be used to support inference for staging
annotations, by automatically inserting transports along preservation
isomorphisms during elaboration. The previous
$\sim\!(\mathit{id'}<\!\mathsf{Bool^0}\!>\,<\!\mathsf{true^0}\!>)$ example could
be simply written as $\mathit{id'}\,\mathsf{Bool^0}\,\mathsf{true^0}$ in the
surface language, and elaboration would transport $\mathit{id'}$ appropriately.
We demonstrated the practical feasibility of such stage inference in a prototype
implementation.

\textbf{Induction on $\Code$}. Basic 2LTT supports \emph{any model} of the
object theory in the presheaf semantics, it does not assume that we have
presheaves over the initial model (syntax). Hence, $\Code\,A$ is a black box
without elimination principles. However, if we are interested in staged
compilation, we can assume the object-level to be syntactic and consistently add
operations on $\Code\,A$ which rely on that assumption, e.g. conversion
checking, pattern matching, or induction on normal forms of object-level
expressions.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
